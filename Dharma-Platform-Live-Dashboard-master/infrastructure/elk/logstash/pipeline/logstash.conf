input {
  beats {
    port => 5044
  }
  
  # Direct log input from services
  tcp {
    port => 5000
    codec => json_lines
  }
  
  # Kafka input for streaming logs
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["dharma-logs"]
    codec => json
    group_id => "logstash-consumer"
  }
}

filter {
  # Parse timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
  }
  
  # Add service identification
  if [service] {
    mutate {
      add_tag => [ "%{service}" ]
    }
  }
  
  # Parse log levels
  if [level] {
    mutate {
      uppercase => [ "level" ]
    }
  }
  
  # Extract correlation IDs for distributed tracing
  if [correlation_id] {
    mutate {
      add_field => { "trace_id" => "%{correlation_id}" }
    }
  }
  
  # Parse error messages and stack traces
  if [level] == "ERROR" {
    if [message] =~ /Traceback/ {
      multiline {
        pattern => "^Traceback"
        negate => true
        what => "previous"
      }
    }
  }
  
  # Grok patterns for common log formats
  grok {
    match => { 
      "message" => "%{TIMESTAMP_ISO8601:log_timestamp} \[%{LOGLEVEL:log_level}\] %{DATA:logger_name}: %{GREEDYDATA:log_message}"
    }
    tag_on_failure => ["_grokparsefailure"]
  }
  
  # Add geolocation for IP addresses if present
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Remove sensitive information
  mutate {
    remove_field => [ "password", "token", "secret", "api_key" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "dharma-logs-%{+YYYY.MM.dd}"
    template_name => "dharma-logs"
    template => "/usr/share/logstash/templates/dharma-logs-template.json"
    template_overwrite => true
  }
  
  # Output to stdout for debugging
  stdout {
    codec => rubydebug
  }
  
  # Send critical errors to dead letter queue
  if [level] == "CRITICAL" or [level] == "FATAL" {
    kafka {
      bootstrap_servers => "kafka:9092"
      topic_id => "dharma-critical-logs"
      codec => json
    }
  }
}